{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "\n",
    "## What is Hugging Face and how does it differ from other ML libraries?\n",
    "\n",
    "Hugging Face provides:\n",
    "- Pre-trained transformer models for NLP/CV/Audio\n",
    "- Model Hub for sharing and discovering models\n",
    "- Datasets library for ML datasets\n",
    "- Tools for training and fine-tuning models\n",
    "- Inference API and model deployment\n",
    "- AutoML capabilities with AutoTrain\n",
    "- Spaces for ML app deployment\n",
    "\n",
    "Key differences from other frameworks:\n",
    "- Focus on transformer architectures\n",
    "- Largest collection of pre-trained models\n",
    "- Stronger community and sharing features\n",
    "- Better standardization across models\n",
    "- Simpler fine-tuning workflows\n",
    "- Integrated deployment solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different kinds of models available:\n",
    "\n",
    "- Multimodal models \n",
    "    - Audio-Text-to-Text, Image-Text-to-Text, Visual Question Answering, Document Question Answering, Video-Text-to-Text, Any-to-Any\n",
    "- Computer Vision models\n",
    "    - Depth Estimation, Image Classification, Object Detection, Image Segmentation, Text-to-Image, Image-to-Text, Image-to-Image, Image-to-Video, Unconditional Image Generation, Video Classification, Text-to-Video, Zero-Shot Image Classification, Mask Generation, Zero-Shot Object Detection, Text-to-3D, Image-to-3D, Image Feature Extraction, Keypoint Detection\n",
    "- Natural Language Processing models\n",
    "    - Text Classification, Token Classification, Table Question Answering, Question Answering, Zero-Shot Classification, Translation, Summarization, Feature Extraction, Text Generation, Text2Text Generation, Fill-Mask, Sentence Similarity, Audio, Text-to-Speech, Text-to-Audio, Automatic Speech Recognition, Audio-to-Audio, Audio Classification, Voice Activity Detection\n",
    "- Tabular models\n",
    "    - Tabular Classification, Tabular Regression, Time Series Forecasting\n",
    "- Reinforcement Learning models\n",
    "    - Reinforcement Learning, Robotics\n",
    "- Other models\n",
    "    - Graph Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you install this?\n",
    "\n",
    "`!pip install transformers datasets evaluate accelerate torch sentencepiece sacremoses -U`\n",
    "\n",
    "- transformers -> library for all kinds of NLP tasks\n",
    "- datasets -> library for all kinds of datasets\n",
    "- evaluate -> library for evaluation of models\n",
    "- accelerate -> library for distributed training\n",
    "- torch -> PyTorch library\n",
    "- sentencepiece -> library for tokenization\n",
    "- sacremoses -> library for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets evaluate accelerate torch sentencepiece sacremoses -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can you use pipelines?\n",
    "\n",
    "Pipelines are a high-level API for using pre-trained models for common NLP tasks. They are easy to use and require minimal code. You can select a task and a model, and then use the pipeline to perform the task.\n",
    "\n",
    "Some parameters are `model=\"distilgpt2\"`, `max_length=20`, `num_return_sequences=2`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998069405555725}, {'label': 'NEGATIVE', 'score': 0.9967179894447327}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\") # distilbert-base-uncased-finetuned-sst-2-english model is used by default\n",
    "print(classifier([\"I love Transformers!\", \"I hate bugs.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is a course about NLP models.', 'labels': ['technology', 'education', 'sports'], 'scores': [0.9376305937767029, 0.05547419190406799, 0.006895218510180712]}\n"
     ]
    }
   ],
   "source": [
    "zero_shot = pipeline(\"zero-shot-classification\")\n",
    "print(zero_shot(\"This is a course about NLP models.\", candidate_labels=[\"education\", \"technology\", \"sports\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Transformers are great for a variety of needs and you can find thousands of them out of the closet'}, {'generated_text': 'Transformers are great for creating exciting, entertaining content for your visitors, who are looking for more entertaining'}]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "print(generator(\"Transformers are great for\", max_length=20, num_return_sequences=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.176646888256073, 'token': 481, 'token_str': ' free', 'sequence': 'Hugging Face is a free library.'}, {'score': 0.07091348618268967, 'token': 285, 'token_str': ' public', 'sequence': 'Hugging Face is a public library.'}]\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model='distilroberta-base')\n",
    "print(unmasker(\"Hugging Face is a <mask> library.\", top_k=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\Sharat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'ORG', 'score': 0.8907568, 'word': 'Hugging Face', 'start': 0, 'end': 12}, {'entity_group': 'LOC', 'score': 0.9991805, 'word': 'New York City', 'start': 25, 'end': 38}]\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True) # Named Entity Recognition\n",
    "print(ner(\"Hugging Face is based in New York City.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9694607853889465, 'start': 25, 'end': 38, 'answer': 'New York City'}\n"
     ]
    }
   ],
   "source": [
    "qa = pipeline(\"question-answering\")\n",
    "print(qa(question=\"Where is Hugging Face based?\", context=\"Hugging Face is based in New York City.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Hugging Face creates tools for N'}]\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "print(summarizer(\"Hugging Face creates tools for NLP. These tools are widely used in AI and ML applications.\", min_length=6, max_length=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Hugging Face is a popular library for the NLP.'}]\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "print(translator(\"Hugging Face est une bibliothèque populaire pour le NLP.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you load and use pre-trained models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Basic tokenization and inference\n",
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Using pipelines (high-level API)\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love this movie!\")\n",
    "\n",
    "ner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "entities = ner(\"My name is Sarah and I live in London\")\n",
    "\n",
    "qa = pipeline(\"question-answering\")\n",
    "result = qa(question=\"Who was Jim Henson?\",\n",
    "           context=\"Jim Henson was a puppeteer\")\n",
    "\n",
    "# Specific task models\n",
    "classifier = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=9\n",
    ")\n",
    "\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    'bert-base-uncased'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
