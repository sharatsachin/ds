{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "### What is Linear Regression?\n",
    "\n",
    "Linear regression can be used to fit a model to an observed data set of values of the resonse (dependent) variable $y$ at different values of the predictor (independent) variable $x$. The model takes the form of a linear equation. The goal is to find the best-fitting straight line through the points.\n",
    "\n",
    "### What is the nomenclature for input variables, output variables, training example, set?\n",
    "\n",
    "- Input variables: $x^{(i)}$ is the vector of input variables for the i-th training example, $x^{(i)} = [x_0^{(i)}, x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]_{(n + 1) \\times 1}$, where $x_0^{(i)} = 1$ and $n$ is the number of features.\n",
    "- Output variables: $y^{(i)}$ is the output variable for the i-th training example.\n",
    "- Training example: $(x^{(i)}, y^{(i)})$ is the i-th training example.\n",
    "- Training set: $\\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})\\}$ is the training set of m examples.\n",
    "\n",
    "### What is the goal in linear regression?\n",
    "\n",
    "The goal is to learn a function $h(x) : x \\rightarrow y$ so that $h(x)$ is a \"good\" predictor for the corresponding value of $y$.\n",
    "\n",
    "### What is the hypothesis function in linear regression? (for the $i^{th}$ training example)\n",
    "\n",
    "The hypothesis function $$ y^{(i)} = h(x^{(i)}) = \\theta_0 + \\theta_1 x_1^{(i)} + \\theta_2 x_2^{(i)} + ... + \\theta_n x_n^{(i)} = \\sum_{j=0}^n \\theta_j x_j^{(i)} = \\theta^T x^{(i)} $$\n",
    "\n",
    "### What is the type of regression called for $n=1$ and $n>1$?\n",
    "\n",
    "- For $n=1$: Simple linear regression $$ y = \\theta_0 + \\theta_1 x $$\n",
    "- For $n>1$: Multiple linear regression $$ y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n $$\n",
    "\n",
    "### What is multivariate linear regression?\n",
    "\n",
    "Multivariate linear regression refers to the case where there are multiple dependent variables and multiple independent variables. $$ y_1, y_2, ..., y_m = f(x_1, x_2, ..., x_n) $$\n",
    "\n",
    "### What is the cost function in linear regression? What is it called?\n",
    "\n",
    "The cost function for linear regression is the Mean Squared Error (MSE) or the sum of squared errors (SSE) over the training set. $$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h(x^{(i)}) - y^{(i)})^2 $$\n",
    "This is the Ordinary Least Squares (OLS) cost function, working to minimize the mean squared error.\n",
    "\n",
    "### Rewriting the linear equation in matrix form, what would it look like?\n",
    "\n",
    "In matrix form, the hypothesis function can be written:\n",
    "\n",
    "$$X = \\begin{bmatrix} - \\left( x^{(1)} \\right)^T - \\\\ - \\left( x^{(2)} \\right)^T - \\\\ \\vdots \\\\ - \\left( x^{(m)} \\right)^T - \\end{bmatrix}_{(m \\times (n+1))} , \\qquad \\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix}_{((n+1) \\times 1)} \\qquad and \\qquad y = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix} _{(m \\times 1)}$$\n",
    "\n",
    "Then the vector of predictions, \n",
    "\n",
    "$$\\hat{y} = X\\theta = \\begin{bmatrix} - \\left( x^{(1)} \\right)^T\\theta - \\\\ - \\left( x^{(2)} \\right)^T\\theta - \\\\ \\vdots \\\\ - \\left( x^{(m)} \\right)^T\\theta - \\end{bmatrix}_{(m \\times 1)}$$\n",
    "\n",
    "### Rewriting the cost function in matrix form, what would it look like?\n",
    "\n",
    "In matrix form, the cost function can be written:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} (X\\theta - y)^T (X\\theta - y)$$\n",
    "\n",
    "### How do you derive the Normal Equation for linear regression?\n",
    "\n",
    "The normal equation is an analytical solution to the linear regression problem with a ordinary least square cost function. That is, to find the value of $\\theta$ that minimizes $J(\\theta)$, take the gradient of $J(\\theta)$ with respect to $\\theta$ and equate to $0$, i.e. $\\delta_{\\theta} J(\\theta) = 0$.\n",
    "\n",
    "### What is the formula for the Normal Equation?\n",
    "\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "## How does gradient descent work for linear regression?\n",
    "\n",
    "Gradient descent is based on the observation that if the function $J({\\theta})$ is differentiable in a neighborhood of a point $\\theta$, then $J({\\theta})$ decreases fastest if one goes from $\\theta$ in the direction of the negative gradient of $J({\\theta})$ at $\\theta$. \n",
    "\n",
    "Thus if we repeatedly apply the following update rule, ${\\theta := \\theta - \\alpha \\nabla J(\\theta)}$ for a sufficiently small value of **learning rate**, $\\alpha$, we will eventually converge to a value of $\\theta$ that minimizes $J({\\theta})$.\n",
    "\n",
    "## What is the update rule for a specific parameter $\\theta_j$?\n",
    "\n",
    "For a specific paramter $\\theta_j$, the update rule is \n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J({\\theta}) $$\n",
    "\n",
    "Using the definition of $J({\\theta})$, we get\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J({\\theta}) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "Therefore, we repeatedly apply the following update rule (batch gradient descent) for all parameters $\\theta_j$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\text{Loop:} \\\\\n",
    "& \\quad \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} \\\\\n",
    "& \\quad \\text{simultaneously update } \\theta_j \\text{ for all } j \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## What is the alternative to batch gradient descent?\n",
    "\n",
    "An alternative to batch gradient descent is stochastic gradient descent (SGD), where we update the parameters using the gradient of the error for each training example.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\text{Loop:} \\\\\n",
    "& \\quad \\text{for } i=1 \\text{ to } m \\text{ \\{} \\\\\n",
    "& \\quad \\quad \\theta_j := \\theta_j - \\alpha \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} \\\\\n",
    "& \\quad \\quad \\text{simultaneously update } \\theta_j \\text{ for all } j \\\\\n",
    "& \\quad \\text{\\}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## What is the learning rate in gradient descent?\n",
    "\n",
    "The learning rate, $\\alpha$, is a hyperparameter that controls how much we are adjusting the weights of our network with respect to the loss gradient. It is a scalar value, typically between $0$ and $1$. If the learning rate is too small, the model will take a long time to converge; if it is too large, the model may overshoot the minimum.\n",
    "\n",
    "## What are the different types of gradient descent?\n",
    "\n",
    "The method that looks at every example in the entire training set on every step is called **batch gradient descent (BGD)**. \n",
    "\n",
    "When the cost function $J$ is convex, all local minima are also global minima, so in this case gradient descent can converge to the global solution.\n",
    "\n",
    "There is also **stochastic gradient descent (SGD)** (also incremental gradient descent), where we repeatedly run through the training set, and for each training example, we update the parameters using gradient of the error for that training example only.\n",
    "\n",
    "## What are the trade-offs between batch and stochastic gradient descent?\n",
    "\n",
    "Whereas BGD has to scan the entire training set before taking a single step, SGD can start making progress right away with each example it looks at. \n",
    "\n",
    "Often, SGD gets $\\theta$ *close* to the minimum much faster than BGD. However it may never *converge* to the minimum, and $\\theta$ will keep oscillating around the minimum of $J(\\theta)$; but in practice these values are reasonably good approximations. Also, by slowly decreasing $\\alpha$ to $0$ as the algorithm runs, $\\theta$ converges to the global minimum rather than oscillating around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual theta: [2 3 4 5 6]\n",
      "Normal Equation theta: [2. 3. 4. 5. 6.]\n",
      "Batch Gradient Descent theta: [2.1  2.85 4.07 5.29 5.51]\n",
      "Stochastic Gradient Descent theta: [2.1  2.84 4.07 5.29 5.51]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# an example of multiple linear regression\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 5  # number of features\n",
    "m = 10  # number of training examples\n",
    "\n",
    "X = np.random.rand(m, n)\n",
    "theta_actual = np.array([2, 3, 4, 5, 6])\n",
    "y = X @ theta_actual\n",
    "\n",
    "\n",
    "def normal_equation(X, y):\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "\n",
    "def batch_gradient_descent(X, y, theta, alpha=0.01, iterations=3000):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    for _ in range(iterations):\n",
    "        h = X @ theta\n",
    "        gradients = (1 / m) * X.T @ (h - y)\n",
    "        theta = theta - alpha * gradients\n",
    "        cost = (1 / (2 * m)) * np.sum((h - y) ** 2)\n",
    "        cost_history.append(cost)\n",
    "    return theta, cost_history[:300]\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(X, y, theta, alpha=0.01, iterations=300):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    for _ in range(iterations):\n",
    "        for i in range(m):\n",
    "            xi = X[i : i + 1]\n",
    "            yi = y[i : i + 1]\n",
    "            h = xi @ theta\n",
    "            gradient = xi.T @ (h - yi)\n",
    "            theta = theta - alpha * gradient\n",
    "            cost = (1 / (2 * m)) * np.sum((h - yi) ** 2)\n",
    "            cost_history.append(cost)\n",
    "    return theta, cost_history[:300]\n",
    "\n",
    "\n",
    "# Run both gradient descent methods\n",
    "theta_nor = normal_equation(X, y)\n",
    "theta_bgd, cost_history_bgd = batch_gradient_descent(X, y, np.zeros(n))\n",
    "theta_sgd, cost_history_sgd = stochastic_gradient_descent(X, y, np.zeros(n))\n",
    "\n",
    "print(\"Actual theta:\", theta_actual)\n",
    "print(\"Normal Equation theta:\", theta_nor.round(2))\n",
    "print(\"Batch Gradient Descent theta:\", theta_bgd.round(2))\n",
    "print(\"Stochastic Gradient Descent theta:\", theta_sgd.round(2))\n",
    "\n",
    "plt.plot(cost_history_bgd, label=\"Batch Gradient Descent\")\n",
    "plt.plot(cost_history_sgd, label=\"Stochastic Gradient Descent\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is regularization used in linear regression? Types of regularization?\n",
    "\n",
    "Regularization is a technique to reduce overfitting in machine learning. This technique discourages learning a more complex or flexible model, by shrinking the parameters towards $0$.\n",
    "\n",
    "We can regularize machine learning methods through the cost function using $L1$ regularization or $L2$ regularization. $L1$ regularization adds an absolute penalty term to the cost function, while $L2$ regularization adds a squared penalty term to the cost function. A model with $L1$ norm for regularisation is called **lasso regression**, and one with (squared) $L2$ norm for regularisation is called **ridge regression**. [Link](https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261)\n",
    "\n",
    "## What is the formula for the cost function with regularization (L1 & L2)?\n",
    "\n",
    "$$J(\\theta)_{L1} = \\frac{1}{2m} \\left( \\sum_{i=1}^m \\left( h_\\theta\\left( x^{(i)} \\right) - y^{(i)} \\right)^2 \\right) + \\frac{\\lambda}{2m} \\left( \\sum_{j=1}^n |\\theta_j| \\right)$$\n",
    "\n",
    "$$J(\\theta)_{L2} = \\frac{1}{2m} \\left( \\sum_{i=1}^m \\left( h_\\theta\\left( x^{(i)} \\right) - y^{(i)} \\right)^2 \\right) + \\frac{\\lambda}{2m} \\left( \\sum_{j=1}^n \\theta_j^2 \\right)$$\n",
    "\n",
    "## What is the partial derivative of the cost function with respect to $\\theta$ for L1 regularization?\n",
    "\n",
    "The partial derivative of the cost function for lasso linear regression is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\frac{\\partial J(\\theta)_{L1}}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} \n",
    "& \\qquad \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial J(\\theta)_{L1}}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} + \\frac{\\lambda}{2m} signum (\\theta_j)\n",
    "& \\qquad \\text{for } j \\ge 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "These equations can be substituted into the general gradient descent update rule to get the specific lasso / ridge update rules.\n",
    "\n",
    "## What is the partial derivative of the cost function with respect to $\\theta$ for L2 regularization?\n",
    "\n",
    "The partial derivative of the cost function for ridge linear regression is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\frac{\\partial J(\\theta)_{L2}}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} \n",
    "& \\qquad \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial J(\\theta)_{L2}}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} + \\frac{\\lambda}{m} \\theta_j \n",
    "& \\qquad \\text{for } j \\ge 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "These equations can be substituted into the general gradient descent update rule to get the specific lasso / ridge update rules.\n",
    "\n",
    "## How does Lasso regression differ from Ridge regression?\n",
    "\n",
    "Lasso regression uses the $L1$ regularization term, which adds an absolute penalty term to the cost function. This leads to sparsity in the model, as it can shrink some coefficients to zero, effectively performing feature selection.\n",
    "Ridge regression uses the $L2$ regularization term, which adds a squared penalty term to the cost function. This leads to all coefficients being shrunk by the same factor, but none are eliminated.\n",
    "\n",
    "## When to use Lasso and when to use Ridge regression?\n",
    "\n",
    "Use Lasso regression when you believe that many features are irrelevant or when you want to perform feature selection. Use Ridge regression when you believe that most features are important and you want to avoid overfitting.\n",
    "\n",
    "## What is Elastic Net regularization?\n",
    "\n",
    "Elastic Net regularization is a combination of $L1$ and $L2$ regularization. It adds both the absolute and squared penalty terms to the cost function, with two hyperparameters $\\lambda_1$ and $\\lambda_2$ to control the strength of each regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated theta: [50.05  5.33  7.88 71.39 76.02]\n",
      "Lasso coefficients: [36.94  0.    0.   62.73 65.22]\n",
      "Ridge coefficients: [44.58  4.43  6.03 66.01 68.68]\n",
      "Elastic Net coefficients: [ 6.62  0.   -0.03 13.03 11.36]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1)\n",
    "\n",
    "theta_calc = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(\"Calculated theta:\", theta_calc.round(2))\n",
    "\n",
    "lasso = Lasso(alpha=10.0)\n",
    "lasso.fit(X, y)\n",
    "print(\"Lasso coefficients:\", lasso.coef_.round(2))\n",
    "\n",
    "ridge = Ridge(alpha=10.0)  # alpha is the regularization strength\n",
    "ridge.fit(X, y)\n",
    "print(\"Ridge coefficients:\", ridge.coef_.round(2))\n",
    "\n",
    "elastic_net = ElasticNet(alpha=10.0, l1_ratio=0.5)\n",
    "elastic_net.fit(X, y)\n",
    "print(\"Elastic Net coefficients:\", elastic_net.coef_.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of assumptions\n",
    "\n",
    "| Assumption | Violation Indicators | Tests | Fixes |\n",
    "| --- | --- | --- | --- |\n",
    "| Linearity - the relationship between the independent and dependent variables is linear | Residuals show a pattern (e.g., a curve) | Plot residuals vs fitted values | Include polynomial terms (e.g., $X^2$, $X^3$) or transform variables (e.g., log, square root) |\n",
    "| Independence - the residuals are independent of each other | Residuals show a pattern over time | Durbin-Watson test (2 -> no autocorrelation, 0-2 -> +ve autocorrelation, 2-4 -> -ve autocorrelation) | Add lag terms / use time series models like ARIMA |\n",
    "| Normality - the residuals follow a normal distribution | Residuals do not follow a normal distribution | Q-Q plot, Shapiro-Wilk test, histogram | Transform variables or consider robust regression |\n",
    "| Equal Variance - the residuals have constant variance | Residuals have non-constant variance | Plot residuals vs fitted values, it will show a cone shape / Breusch-Pagan test | Transform response variable (eg. log, square root) or use weighted least squares |\n",
    "| No Multicollinearity - the independent variables are not highly correlated | High correlation between predictors | Variance Inflation Factor (VIF > 5-10 indicates high correlation) | Remove or combine correlated variables |\n",
    "| No Endogeneity - the independent variables are not correlated with the residuals | Predictors correlated with residuals (can happen when omitted variable bias or simultaneity when relationship is bidirectional) | Plot residuals vs predictors | Use instrumental variables or 2SLS (Two-Stage Least Squares) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What will happen (challenges) if we try to predict categorical values using linear regression? (Using a threshold)\n",
    "\n",
    "1. Violates assumptions of linear regression.\n",
    "2. Predictions may fall outside valid range (e.g., probabilities < 0 or > 1).\n",
    "3. Threshold choice affects results significantly.\n",
    "4. Poor performance compared to logistic regression for binary outcomes.\n",
    "\n",
    "### Why we divide the value by 2 in loss function?\n",
    "\n",
    "Dividing by 2 simplifies the derivative calculation, making optimization easier without changing the optimal parameters.\n",
    "\n",
    "### Why we use square & root under in loss function why not take the absolute value?\n",
    "\n",
    "Squaring makes the function differentiable everywhere, unlike absolute value. It also penalizes larger errors more heavily.\n",
    "\n",
    "### How do we optimize the loss function?\n",
    "\n",
    "Gradient descent: Iteratively update parameters in the direction of steepest decrease in the loss function.\n",
    "\n",
    "### What is $R^2$ and importance of it?\n",
    "\n",
    "R-squared (R) measures the proportion of variance in the dependent variable explained by the independent variables. It ranges from 0 to 1, with higher values indicating better fit.\n",
    "\n",
    "### What is the formula for $R^2$?\n",
    "\n",
    "$$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} $$\n",
    "\n",
    "Where $SS_{res} = \\sum_{i=1}^m (y^{(i)} - h(x^{(i)}))^2$ is the sum of squared residuals and $SS_{tot} = \\sum_{i=1}^m (y^{(i)} - \\bar{y})^2$ is the total sum of squares.\n",
    "\n",
    "### What is the difference between $R^2$ & Adjusted $R^2$?\n",
    "\n",
    "Adjusted R penalizes the addition of unnecessary predictors, while R always increases with more predictors. Adjusted R is more suitable for comparing models with different numbers of predictors.\n",
    "\n",
    "### What is the formula for Adjusted $R^2$?\n",
    "\n",
    "$$ \\text{Adjusted } R^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1} $$\n",
    "\n",
    "Where $n$ is the number of observations and $k$ is the number of predictors.\n",
    "\n",
    "### How do we handle over fitting problem?\n",
    "\n",
    "1. Regularization\n",
    "2. Feature selection\n",
    "3. Cross-validation\n",
    "4. Increasing training data\n",
    "5. Reducing model complexity\n",
    "\n",
    "### What are the different metrics for the model evaluation?\n",
    "\n",
    "1. MSE/RMSE\n",
    "2. MAE\n",
    "3. R-squared\n",
    "4. Adjusted R-squared\n",
    "5. F-statistic\n",
    "6. AIC/BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different metrics for regression\n",
    "\n",
    "| **Metric Name**       | **Use Case**                                                                 | **Formula**                                                                 | **Advantages**                                                                 | **Disadvantages**                                                              | **Gotchas**                                                                 |\n",
    "|-----------------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------|-----------------------------------------------------------------------------|\n",
    "| **MAE** (Mean Absolute Error) | Robust evaluation with outliers present                                    | $\\frac{1}{m}\\sum\\|y - \\hat{y}\\|$                                            | - Robust to outliers<br>- Intuitive interpretation                            | - Not differentiable at zero<br>- Less emphasis on large errors               | - Same unit as target variable<br>- Less sensitive than MSE                 |\n",
    "| **MSE** (Mean Squared Error) | Emphasizing large errors (e.g., financial risk modeling)                   | $\\frac{1}{m}\\sum(y - \\hat{y})^2$                                            | - Differentiable<br>- Punishes large errors                                   | - Sensitive to outliers<br>- Units squared                                    | - Can be numerically large<br>- Harder to interpret<br> - Optimizes towards mean                         |\n",
    "| **RMSE** (Root Mean Squared Error) | Error interpretation in original units                                    | $\\sqrt{\\frac{1}{m}\\sum(y - \\hat{y})^2}$                                     | - Same units as target<br>- Reflects error magnitude                          | - Still sensitive to outliers                                                 | - Sensitive to sample size                                                  |\n",
    "| **R** (R-Squared)    | Explaining variance proportion                                             | $1 - \\frac{\\sum(y - \\hat{y})^2}{\\sum(y - \\bar{y})^2}$                       | - Scale-independent (0-1)<br>- Relative performance measure                   | - Misleading for nonlinear models<br>- Increases with more predictors         | - Can be negative for poor models<br>- Doesn't indicate bias                |\n",
    "| **MAE%** (Mean Absolute Error Percentage) | Relative error assessment (e.g., business forecasting)                    | $100 \\times \\frac{\\sum\\|y - \\hat{y}\\|}{\\sum y}$                              | - Intuitive percentage interpretation                                         | - Undefined for zero values<br>- Asymmetric penalties                         | - Biased toward underestimation if $y$ varies widely<br>- Optimizes towards median                       |\n",
    "| **MAPE** (Mean Absolute Percentage Error) | Relative error assessment (e.g., business forecasting)                    | $\\frac{100\\%}{m}\\sum\\|\\frac{y - \\hat{y}}{y}\\|$                              | - Intuitive percentage interpretation                                         | - Undefined for zero values<br>- Asymmetric penalties                         | - Biased toward underestimation if $y$ varies widely                       |\n",
    "| **Huber Loss**        | Balance between MAE/MSE (e.g., robust regression)                          | $\\begin{cases}0.5(y - \\hat{y})^2 & \\text{if } \\|y - \\hat{y}\\| \\leq \\delta \\\\ \\delta\\|y - \\hat{y}\\| - 0.5\\delta^2 & \\text{otherwise}\\end{cases}$ | - Robust yet differentiable<br>- Customizable via $\\delta$                    | - Requires tuning $\\delta$<br>- More complex implementation                  | - $\\delta$ typically chosen via cross-validation                           |\n",
    "| **MSLE** (Mean Squared Logarithmic Error) | Targets with exponential trends (e.g., population growth)                 | $\\frac{1}{m}\\sum(\\log(y+1) - \\log(\\hat{y}+1))^2$                            | - Reduces impact of large errors<br>- Handles exponential scales               | - Sensitive to small errors<br>- Cannot handle negative values                | - Requires $y \\geq 0$<br>- Penalizes underestimation more                  |\n",
    "| **RMSLE**             | Interpretable version of MSLE                                              | $\\sqrt{\\frac{1}{m}\\sum(\\log(y+1) - \\log(\\hat{y}+1))^2}$                     | - Preserves MSLE benefits with original unit intuition                        | - Same limitations as MSLE                                                    | - Sensitive to log-scale offsets                                           |\n",
    "| **Quantile Loss**     | Predicting intervals/uncertainty (e.g., financial risk)                   | $\\frac{1}{m}\\sum \\begin{cases} \\tau\\|y - \\hat{y}\\| & y \\geq \\hat{y} \\\\ (1-\\tau)\\|y - \\hat{y}\\| & y < \\hat{y} \\end{cases}$ | - Flexible for different risk preferences<br>- Non-parametric                 | - Computationally intensive<br>- Requires choosing $\\tau$                     | - $\\tau=0.5$ reduces to MAE<br>- Asymmetric by design                      |\n",
    "| **Tweedie Deviance**  | Modeling semi-continuous data (e.g., insurance claims)                     | $2\\left(\\frac{y^{2-p}}{(1-p)(2-p)} - \\frac{y\\hat{y}^{1-p}}{1-p} + \\frac{\\hat{y}^{2-p}}{2-p}\\right)$ | - Handles zero-inflated data<br>- Flexible power parameter ($p$)              | - Complex implementation<br>- Requires tuning $p$                             | - $p=0$ (Normal), $p=1$ (Poisson), $p=2$ (Gamma), $p=3$ (Inverse Gaussian) |\n",
    "| **Max Error**         | Worst-case scenario analysis                                               | $\\max(\\|y - \\hat{y}\\|)$                                                     | - Simple worst-case bound                                                     | - Ignores distribution of errors<br>- Single-point dependency                 | - Highly sensitive to outliers                                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining regression results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  GRADE   R-squared:                       0.416\n",
      "Model:                            OLS   Adj. R-squared:                  0.353\n",
      "Method:                 Least Squares   F-statistic:                     6.646\n",
      "Date:                Sat, 25 Jan 2025   Prob (F-statistic):            0.00157\n",
      "Time:                        11:12:58   Log-Likelihood:                -12.978\n",
      "No. Observations:                  32   AIC:                             33.96\n",
      "Df Residuals:                      28   BIC:                             39.82\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "GPA            0.4639      0.162      2.864      0.008       0.132       0.796\n",
      "TUCE           0.0105      0.019      0.539      0.594      -0.029       0.050\n",
      "PSI            0.3786      0.139      2.720      0.011       0.093       0.664\n",
      "const         -1.4980      0.524     -2.859      0.008      -2.571      -0.425\n",
      "==============================================================================\n",
      "Omnibus:                        0.176   Durbin-Watson:                   2.346\n",
      "Prob(Omnibus):                  0.916   Jarque-Bera (JB):                0.167\n",
      "Skew:                           0.141   Prob(JB):                        0.920\n",
      "Kurtosis:                       2.786   Cond. No.                         176.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "spector_data = sm.datasets.spector.load()\n",
    "spector_data.exog = sm.add_constant(spector_data.exog, prepend=False)\n",
    "mod = sm.OLS(spector_data.endog, spector_data.exog)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do the coef, std. error, t-statistic, p-value, and 95% confidence intervals represent in the summary output?\n",
    "\n",
    "1. **Coef**: Estimated coefficients for the predictors. Represents the change in the dependent variable for a one-unit change in the predictor.\n",
    "2. **Std. Error**: Standard error of the coefficient estimate. Measures the variability in the estimate. A lower value indicates a more precise estimate.\n",
    "3. **t-statistic**: The coefficient divided by its standard error. Measures the significance of the coefficient estimate. Value greater than 2 indicates significance.\n",
    "4. **P-value**: Probability of observing the t-statistic if the null hypothesis is true. Typically, p < 0.05 is considered significant.\n",
    "5. **95% Confidence Intervals**: Range of values within which the true coefficient is likely to fall with 95% confidence.\n",
    "\n",
    "### What is the formula for F-statistic?\n",
    "\n",
    "The F-statistic is used to test the overall significance of the model as compared to a model with no predictors. It is calculated as:\n",
    "\n",
    "$$ F = \\frac{(TSS - RSS) / p}{RSS / (n - p - 1)} $$\n",
    "\n",
    "Where $TSS = \\sum_{i=1}^m (y^{(i)} - \\bar{y})^2$ is the total sum of squares, $RSS = \\sum_{i=1}^m (y^{(i)} - h(x^{(i)}))^2$ is the residual sum of squares, $p$ is the number of predictors, and $n$ is the number of observations. Here $n - p - 1$ is the degrees of freedom.\n",
    "\n",
    "It's range is from 0 to infinity. Higher values indicate a better fit.\n",
    "\n",
    "### What is Prob(F-statistic)?\n",
    "\n",
    "The probability associated with the F-statistic is the probability of observing an F-statistic as extreme as the one computed from the data, assuming the null hypothesis is true. It is used to test the overall significance of the model.\n",
    "\n",
    "In this case, the null hypothesis is that all coefficients are zero, i.e., the model has no predictive power.\n",
    "\n",
    "A smaller values, typically less than 0.05, indicates that the model is significant.\n",
    "\n",
    "### What is the formula for AIC?\n",
    "\n",
    "AIC (Akaike Information Criterion) is a measure of the relative quality of a statistical model for a given set of data. It is calculated as:\n",
    "\n",
    "$$ AIC = 2k - 2\\ln(L) $$\n",
    "\n",
    "Where $k$ is the number of parameters and $L$ is the likelihood function.\n",
    "\n",
    "### What is the formula for BIC?\n",
    "\n",
    "BIC (Bayesian Information Criterion) is a criterion for model selection among a finite set of models. It is calculated as:\n",
    "\n",
    "$$ BIC = k\\ln(n) - 2\\ln(L) $$\n",
    "\n",
    "Where $n$ is the number of observations.\n",
    "\n",
    "### What do the model diagnostics (Omnibus, Skew, Kurtosis, Jarque-Bera, Durbin-Watson, Condition Number) represent?\n",
    "\n",
    "1. **Omnibus**: Tests the skewness and kurtosis of the residuals. A value close to zero indicates normal distribution. \n",
    "2. **Skew**: Measures the symmetry of the residuals. A value close to zero indicates normal distribution. $ < 0$ indicates negative skew and $ > 0$ indicates positive skew.\n",
    "3. **Kurtosis**: Measures the heaviness of the tails of the residuals. A value close to zero indicates normal distribution. $ < 3$ indicates light tails and $ > 3$ indicates heavy tails.\n",
    "4. **Jarque-Bera**: Tests the skewness and kurtosis of the residuals. A value close to zero indicates normal distribution.\n",
    "5. **Durbin-Watson**: Tests for autocorrelation in the residuals. Value of 2 indicates no autocorrelation, while values < 2 or > 2 indicate positive or negative autocorrelation.\n",
    "6. **Condition Number**: Measures multicollinearity in the model. Values > 30 indicate multicollinearity.\n",
    "\n",
    "### What are the drawbacks of the linear model?\n",
    "\n",
    "1. Assumes linear relationship between predictors and response.\n",
    "2. Sensitive to outliers.\n",
    "3. Assumes independence of observations.\n",
    "4. Assumes homoscedasticity.\n",
    "5. Assumes normality of residuals.\n",
    "6. Prone to overfitting with many predictors.\n",
    "7. Cannot capture non-linear relationships."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
